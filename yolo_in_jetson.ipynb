{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3041a67d-27ea-4d7b-8993-7cbafaf4dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2af6923-aac7-4754-b48f-3cbc618e8942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gstreamer_pipeline(\n",
    "    sensor_id=0,\n",
    "    capture_width=1280,\n",
    "    capture_height=720,\n",
    "    display_width=1280,\n",
    "    display_height=720,\n",
    "    framerate=60,\n",
    "    flip_method=0,\n",
    "):\n",
    "    return (\n",
    "        \"nvarguscamerasrc sensor-id=%d ! \"\n",
    "        \"video/x-raw(memory:NVMM), width=(int)%d, height=(int)%d, framerate=(fraction)%d/1 ! \"\n",
    "        \"nvvidconv flip-method=%d ! \"\n",
    "        \"video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! \"\n",
    "        \"videoconvert ! \"\n",
    "        \"video/x-raw, format=(string)BGR ! appsink\"\n",
    "        % (\n",
    "            sensor_id,\n",
    "            capture_width,\n",
    "            capture_height,\n",
    "            framerate,\n",
    "            flip_method,\n",
    "            display_width,\n",
    "            display_height,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c95353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(model):\n",
    "    COLORS = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (0, 255, 255), (255, 0, 255)]  # Îã§ÏñëÌïú ÏÉâÏÉÅ\n",
    "    window_title = \"YOLOv5 detection\"\n",
    "    inference_times_mean = []\n",
    "\n",
    "    # To flip the image, modify the flip_method parameter (0 and 2 are the most common)\n",
    "    print(gstreamer_pipeline(flip_method=0))\n",
    "    video_capture = cv2.VideoCapture(gstreamer_pipeline(flip_method=0), cv2.CAP_GSTREAMER)\n",
    "    if video_capture.isOpened():\n",
    "        try:\n",
    "            window_handle = cv2.namedWindow(window_title, cv2.WINDOW_AUTOSIZE)\n",
    "            while True:\n",
    "                ret_val, frame = video_capture.read()\n",
    "                if not ret_val:\n",
    "                    break\n",
    "                start_event = torch.cuda.Event(enable_timing=True)\n",
    "                end_event = torch.cuda.Event(enable_timing=True)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    start_event.record()\n",
    "                    results = model(frame, size=1280, augment=True)\n",
    "                    end_event.record()\n",
    "                    \n",
    "                torch.cuda.synchronize()\n",
    "                elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "                \n",
    "                for *xyxy, conf, cls in results.xyxy[0].tolist():\n",
    "                    x1, y1, x2, y2 = map(int, xyxy)\n",
    "                    label = f\"{results.names[int(cls)]} {conf:.2f}\"\n",
    "                    color = COLORS[int(cls) % len(COLORS)]\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                    label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "                    label_x1, label_y1 = x1, y1 - label_size[1] - 10\n",
    "                    label_x2, label_y2 = x1 + label_size[0] + 10, y1\n",
    "                    cv2.rectangle(frame, (label_x1, label_y1), (label_x2, label_y2), color, -1)\n",
    "                    cv2.putText(frame, label, (x1 + 5, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "                cv2.imshow(window_title, frame)\n",
    "                print('inference time : {}ms'.format(elapsed_time_ms))\n",
    "                inference_times_mean.append(elapsed_time_ms)\n",
    "                keyCode = cv2.waitKey(10) & 0xFF\n",
    "                # Stop the program on the ESC key or 'q'\n",
    "                if keyCode == 27 or keyCode == ord('q'):\n",
    "                    break\n",
    "        finally:\n",
    "            video_capture.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            print(\"\\nMean inference time : {}ms\".format(sum(inference_times_mean[5:]) / len(inference_times_mean[5:])))\n",
    "    else:\n",
    "        print(\"Error: Unable to open camera\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e57ce512-3dbb-467c-9ab6-970bcf69d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def prune(model, amount=0.3):\n",
    "    import torch.nn.utils.prune as prune\n",
    "    print('Pruning model... ', end='')\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, torch.nn.Conv2d):\n",
    "            prune.l1_unstructured(m, name='weight', amount=amount)  # prune\n",
    "            prune.remove(m, 'weight')  # make permanent\n",
    "    print(' %.3g global sparsity' % sparsity(model))\n",
    "    return model\n",
    "            \n",
    "def sparsity(model):\n",
    "    # Return global model sparsity\n",
    "    a, b = 0, 0\n",
    "    for p in model.parameters():\n",
    "        a += p.numel()\n",
    "        b += (p == 0).sum()\n",
    "    return b / a\n",
    "\n",
    "def quantize(model):\n",
    "    model = model.half()  # Î™®Îç∏Ïùò Í∞ÄÏ§ëÏπòÎ•º FP16ÏúºÎ°ú Î≥ÄÌôò\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, torch.nn.BatchNorm2d):\n",
    "            layer.float()  # BatchNorm Î†àÏù¥Ïñ¥Îäî FP32Î°ú Ïú†ÏßÄ\n",
    "    return model\n",
    "\n",
    "def print_model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n",
    "    os.remove('tmp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fcd68085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_prune(model, pruning_ratio):\n",
    "    import torch_pruning as tp\n",
    "    #print(model.model)\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(True)\n",
    "\n",
    "    example_inputs = torch.randn(1, 3, 640, 640).to(device)\n",
    "    imp = tp.importance.MagnitudeImportance(p=2) # L2 norm pruning\n",
    "\n",
    "    ignored_layers = []\n",
    "    from models.yolo import Detect\n",
    "    from models.common import Conv\n",
    "    for m in model.model.modules():\n",
    "        if isinstance(m, Detect):\n",
    "            ignored_layers.append(m)\n",
    "    #print(ignored_layers)\n",
    "\n",
    "    iterative_steps = 1 # progressive pruning\n",
    "    pruner = tp.pruner.MetaPruner(\n",
    "        model.model,\n",
    "        example_inputs,\n",
    "        importance=imp,\n",
    "        global_pruning=True,\n",
    "        iterative_steps=iterative_steps,\n",
    "        pruning_ratio=pruning_ratio, # remove 50% channels, ResNet18 = {64, 128, 256, 512} => ResNet18_Half = {32, 64, 128, 256}\n",
    "        ignored_layers=ignored_layers,\n",
    "        round_to=4,\n",
    "    )\n",
    "\n",
    "    \n",
    "    base_macs, base_nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "    pruner.step()\n",
    "\n",
    "    pruned_macs, pruned_nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "    #print(model)\n",
    "    print(\"Before Pruning: MACs=%f G, #Params=%f G\"%(base_macs/1e9, base_nparams))\n",
    "    print(\"After Pruning: MACs=%f G, #Params=%f G\"%(pruned_macs/1e9, pruned_nparams))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f1107ce-dacb-4e77-8e84-187fa180c226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "320b9607-6315-49cb-90b5-498412c189f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jetson/Documents/Glasses_Detection_with_YOLOv5\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Documents/Glasses_Detection_with_YOLOv5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf766168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ 8c2ab6f torch 1.8.0 CUDA:0 (NVIDIA Tegra X1, 3963MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('yolov5', 'custom', 'finetuned_weights/yolov5n_finetuned.pt', source='local', force_reload=True, device=device)\n",
    "#model = torch.hub.load('ultralytics/yolov5', \"custom\", \"models/best.pt\", force_reload=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6db1a24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning model...  0.499 global sparsity\n"
     ]
    }
   ],
   "source": [
    "model = prune(model, amount=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "603f2ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Pruning: MACs=2.079101 G, #Params=1761871.000000 G\n",
      "After Pruning: MACs=1.491328 G, #Params=1459187.000000 G\n"
     ]
    }
   ],
   "source": [
    "model = real_prune(model, pruning_ratio=0.1)\n",
    "#model = quantize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a83fdc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvarguscamerasrc sensor-id=0 ! video/x-raw(memory:NVMM), width=(int)1280, height=(int)720, framerate=(fraction)60/1 ! nvvidconv flip-method=0 ! video/x-raw, width=(int)1280, height=(int)720, format=(string)BGRx ! videoconvert ! video/x-raw, format=(string)BGR ! appsink\n",
      "inference time : 6156.8359375ms\n",
      "inference time : 5100.8525390625ms\n",
      "inference time : 504.8294677734375ms\n",
      "inference time : 373.4302673339844ms\n",
      "inference time : 355.9434509277344ms\n",
      "inference time : 525.6057739257812ms\n",
      "inference time : 358.2197265625ms\n",
      "inference time : 346.3018798828125ms\n",
      "inference time : 351.9606628417969ms\n",
      "inference time : 351.1159362792969ms\n",
      "inference time : 352.2029724121094ms\n",
      "inference time : 352.6986389160156ms\n",
      "inference time : 348.7313537597656ms\n",
      "inference time : 345.0692138671875ms\n",
      "inference time : 351.6027526855469ms\n",
      "inference time : 347.80078125ms\n",
      "inference time : 351.8130798339844ms\n",
      "inference time : 348.33270263671875ms\n",
      "inference time : 346.4955749511719ms\n",
      "inference time : 350.446044921875ms\n",
      "inference time : 354.01092529296875ms\n",
      "inference time : 370.6042785644531ms\n",
      "inference time : 355.93280029296875ms\n",
      "inference time : 348.3380126953125ms\n",
      "inference time : 349.1613464355469ms\n",
      "inference time : 349.878173828125ms\n",
      "inference time : 346.19073486328125ms\n",
      "inference time : 348.98297119140625ms\n",
      "inference time : 348.9397277832031ms\n",
      "inference time : 348.6639709472656ms\n",
      "inference time : 349.3177185058594ms\n",
      "inference time : 348.1845397949219ms\n",
      "inference time : 356.6557922363281ms\n",
      "inference time : 353.4713439941406ms\n",
      "inference time : 354.7298278808594ms\n",
      "inference time : 351.01885986328125ms\n",
      "inference time : 352.379638671875ms\n",
      "inference time : 351.5200500488281ms\n",
      "inference time : 351.1090087890625ms\n",
      "inference time : 350.2580261230469ms\n",
      "inference time : 348.43865966796875ms\n",
      "\n",
      "Mean inference time : 356.0050972832574ms\n"
     ]
    }
   ],
   "source": [
    "detect(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8f248c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saved 1 image to \u001b[1mtest/result\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time : 3722.6640625ms\n",
      "\n",
      "Mean inference time : 3722.6640625ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "COLORS = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (0, 255, 255), (255, 0, 255)]  # Îã§ÏñëÌïú ÏÉâÏÉÅ\n",
    "inference_times_mean = []\n",
    "image_folder = './test'\n",
    "output_folder = './test/result'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "image_files = list(Path(image_folder).rglob('*.jpg'))\n",
    "model.eval()\n",
    "model.conf=0.01\n",
    "\n",
    "for image_file in image_files:\n",
    "    image_file = Path('./test/test_1.jpg')\n",
    "    img = Image.open(image_file)\n",
    "\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_event.record()\n",
    "        results = model(img, size=1280, augment=True)\n",
    "        end_event.record()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "\n",
    "    results_img_path = os.path.join(output_folder, image_file.name)\n",
    "    results.save(save_dir=output_folder)\n",
    "\n",
    "    print('inference time : {}ms'.format(elapsed_time_ms))\n",
    "    inference_times_mean.append(elapsed_time_ms)\n",
    "    break\n",
    "\n",
    "print(\"\\nMean inference time : {}ms\".format(sum(inference_times_mean) / len(inference_times_mean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b35cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
