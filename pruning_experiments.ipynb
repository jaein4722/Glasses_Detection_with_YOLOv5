{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/Documents/Glasses_Detection_with_YOLOv5/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from utils.datasets import create_dataloader\n",
    "from utils.general import (LOGGER, box_iou, check_dataset, check_img_size, check_requirements, check_yaml,\n",
    "                           coco80_to_coco91_class, colorstr, increment_path, non_max_suppression, print_args,\n",
    "                           scale_coords, xywh2xyxy, xyxy2xywh)\n",
    "from utils.metrics import ConfusionMatrix, ap_per_class\n",
    "from utils.plots import output_to_target, plot_images, plot_val_study\n",
    "from utils.torch_utils import select_device, time_sync\n",
    "import torch_pruning as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_prune(model, pruning_ratio=0.3, real_pruning=False, mode='Structured'):\n",
    "    print('Pruning model with ratio = {}... '.format(pruning_ratio), end='')\n",
    "    if mode == 'Structured':\n",
    "        pruning_ratio = 1 - np.sqrt(1 - pruning_ratio) # for removing amount of parameters / structured pruning removes channel\n",
    "    if not real_pruning:\n",
    "        for name, m in model.named_modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                if mode == 'Structured':\n",
    "                    prune.ln_structured(m, name='weight', amount=pruning_ratio, n=1, dim=1)  # prune\n",
    "                elif mode == 'Unstructured':\n",
    "                    prune.l1_unstructured(m, name='weight', amount=pruning_ratio)  # prune\n",
    "                prune.remove(m, 'weight')  # make permanent\n",
    "                \n",
    "        for p in model.parameters():\n",
    "            p.requires_grad_(True)\n",
    "    else:\n",
    "        #print(model.model)\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        example_inputs = torch.randn(1, 3, 640, 640).to(device)\n",
    "    \n",
    "        ignored_layers = []\n",
    "        from models.yolo import Detect\n",
    "        for m in model.model.modules():\n",
    "            if isinstance(m, Detect):\n",
    "                ignored_layers.append(m)\n",
    "        #print(ignored_layers)\n",
    "\n",
    "        iterative_steps = 1 # progressive pruning\n",
    "        pruner = tp.pruner.GroupNormPruner(\n",
    "            model.model,\n",
    "            example_inputs,\n",
    "            importance=tp.importance.GroupNormImportance(p=1, normalizer=None, group_reduction='sum'),\n",
    "            global_pruning=False,\n",
    "            isomorphic=False,\n",
    "            iterative_steps=iterative_steps,\n",
    "            pruning_ratio=pruning_ratio, # remove 50% channels, ResNet18 = {64, 128, 256, 512} => ResNet18_Half = {32, 64, 128, 256}\n",
    "            ignored_layers=ignored_layers,\n",
    "        )\n",
    "        pruner.step()\n",
    "    print('Pruning end. Calculating model information...')\n",
    "    return model\n",
    "\n",
    "def get_model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    model_size = os.path.getsize(\"tmp.pt\")/1e6\n",
    "    print(\"model_size: %.2f MB\" %(model_size))\n",
    "    os.remove('tmp.pt')\n",
    "    return model_size\n",
    "\n",
    "def flops_and_params(model):\n",
    "    example_inputs = torch.randn(1, 3, 640, 640).to(device)\n",
    "    macs, nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "    flops = 2 * macs\n",
    "    return flops, nparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(detections, labels, iouv):\n",
    "    \"\"\"\n",
    "    Return correct predictions matrix. Both sets of boxes are in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        detections (Array[N, 6]), x1, y1, x2, y2, conf, class\n",
    "        labels (Array[M, 5]), class, x1, y1, x2, y2\n",
    "    Returns:\n",
    "        correct (Array[N, 10]), for 10 IoU levels\n",
    "    \"\"\"\n",
    "    correct = torch.zeros(detections.shape[0], iouv.shape[0], dtype=torch.bool, device=iouv.device)\n",
    "    iou = box_iou(labels[:, 1:], detections[:, :4])\n",
    "    x = torch.where((iou >= iouv[0]) & (labels[:, 0:1] == detections[:, 5]))  # IoU above threshold and classes match\n",
    "    if x[0].shape[0]:\n",
    "        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()  # [label, detection, iou]\n",
    "        if x[0].shape[0] > 1:\n",
    "            matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "            matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
    "            # matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "            matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
    "        matches = torch.Tensor(matches).to(iouv.device)\n",
    "        correct[matches[:, 1].long()] = matches[:, 2:3] >= iouv\n",
    "    return correct\n",
    "\n",
    "def evaluate(model, data_yaml, device='cuda:0', img_size=640, batch_size=32, conf_thres=0.001, iou_thres=0.6, half=True):\n",
    "    \"\"\"\n",
    "    YOLOv5 모델 테스트 함수 (2022년 초 버전 기반).\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): PyTorch YOLOv5 모델 객체\n",
    "        data_yaml (str or Path): 데이터셋 yaml 파일 경로\n",
    "        device (str): 'cuda' 또는 'cpu'\n",
    "        img_size (int): 입력 이미지 크기 (정사각형 기준)\n",
    "        batch_size (int): 배치 크기\n",
    "        conf_thres (float): Confidence threshold\n",
    "        iou_thres (float): IoU threshold for NMS\n",
    "    \n",
    "    Returns:\n",
    "        dict: 테스트 결과 메트릭 (Precision, Recall, mAP@0.5, mAP@0.5:0.95)\n",
    "    \"\"\"\n",
    "    # 디바이스 설정\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # 데이터셋 확인\n",
    "    data = check_dataset(data_yaml)\n",
    "    \n",
    "    model.eval()\n",
    "    is_coco = isinstance(data.get('val'), str) and data['val'].endswith('coco/val2017.txt')  # COCO dataset\n",
    "    nc = int(data['nc'])  # number of classes\n",
    "    iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n",
    "    niou = iouv.numel()\n",
    "    \n",
    "    pad = 0.5\n",
    "    task = 'val'  # path to train/val/test images\n",
    "    dataloader = create_dataloader(data[task], img_size, batch_size, model.stride, False, pad=pad, rect=True,\n",
    "                                    workers=8, prefix=colorstr(f'{task}: '))[0]\n",
    "    \n",
    "    seen = 0\n",
    "    names = {k: v for k, v in enumerate(model.names if hasattr(model, 'names') else model.module.names)}\n",
    "    class_map = list(range(1000))\n",
    "    s = ('%20s' + '%11s' * 6) % ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\n",
    "    dt, p, r, f1, mp, mr, map50, map = [0.0, 0.0, 0.0], 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    loss = torch.zeros(3, device=device)\n",
    "    jdict, stats, ap, ap_class = [], [], [], []\n",
    "    pbar = tqdm(dataloader, desc=s, bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')  # progress bar\n",
    "    \n",
    "    for batch_i, (im, targets, paths, shapes) in enumerate(pbar):\n",
    "        t1 = time_sync()\n",
    "        im = im.to(device, non_blocking=True)\n",
    "        targets = targets.to(device)\n",
    "        im = im.half() if half else im.float()  # uint8 to fp16/32\n",
    "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "        nb, _, height, width = im.shape  # batch size, channels, height, width\n",
    "        t2 = time_sync()\n",
    "        dt[0] += t2 - t1\n",
    "\n",
    "        # Inference\n",
    "        out = model(im, augment=False)  # inference, loss outputs\n",
    "        dt[1] += time_sync() - t2\n",
    "\n",
    "        # NMS\n",
    "        targets[:, 2:] *= torch.Tensor([width, height, width, height]).to(device)  # to pixels\n",
    "        lb = []  # for autolabelling\n",
    "        t3 = time_sync()\n",
    "        out = non_max_suppression(out, conf_thres, iou_thres, labels=lb, multi_label=True, agnostic=False)\n",
    "        dt[2] += time_sync() - t3\n",
    "\n",
    "        # Metrics\n",
    "        for si, pred in enumerate(out):\n",
    "            labels = targets[targets[:, 0] == si, 1:]\n",
    "            nl = len(labels)\n",
    "            tcls = labels[:, 0].tolist() if nl else []  # target class\n",
    "            path, shape = Path(paths[si]), shapes[si][0]\n",
    "            seen += 1\n",
    "\n",
    "            if len(pred) == 0:\n",
    "                if nl:\n",
    "                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
    "                continue\n",
    "\n",
    "            # Predictions\n",
    "            predn = pred.clone()\n",
    "            scale_coords(im[si].shape[1:], predn[:, :4], shape, shapes[si][1])  # native-space pred\n",
    "\n",
    "            # Evaluate\n",
    "            if nl:\n",
    "                tbox = xywh2xyxy(labels[:, 1:5])  # target boxes\n",
    "                scale_coords(im[si].shape[1:], tbox, shape, shapes[si][1])  # native-space labels\n",
    "                labelsn = torch.cat((labels[:, 0:1], tbox), 1)  # native-space labels\n",
    "                correct = process_batch(predn, labelsn, iouv)\n",
    "                \n",
    "            else:\n",
    "                correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool)\n",
    "            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))  # (correct, conf, pcls, tcls)\n",
    "\n",
    "    # Compute metrics\n",
    "    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
    "    if len(stats) and stats[0].any():\n",
    "        tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats, names=names)\n",
    "        ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n",
    "    else:\n",
    "        nt = torch.zeros(1)\n",
    "\n",
    "    # Print results\n",
    "    pf = '%20s' + '%11i' * 2 + '%11.3g' * 4  # print format\n",
    "    print(pf % ('all', seen, nt.sum(), mp, mr, map50, map))\n",
    "\n",
    "    # Print speeds\n",
    "    t = tuple(x / seen * 1E3 for x in dt)  # speeds per image\n",
    "    shape = (batch_size, 3, img_size, img_size)\n",
    "    print(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {shape}' % t)\n",
    "\n",
    "    # 결과 반환\n",
    "    results = {\n",
    "        'Precision': mp,\n",
    "        'Recall': mr,\n",
    "        'mAP@0.5': map50,\n",
    "        'mAP@0.5:0.95': map\n",
    "    }\n",
    "    return (results, t)\n",
    "\n",
    "# Example Usage (adjust paths as needed):\n",
    "# test_yolov5_v1(model, data_yaml='data/coco128.yaml', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pruning(weight='n', real_pruning=False, mode='Unstructured', half=False):\n",
    "    pruning_ratios = np.linspace(0.0, 0.9, 10)  # 0%, 10%, ..., 50%\n",
    "    results = []\n",
    "    inform = []\n",
    "\n",
    "    model = torch.hub.load('.', 'custom', '../finetuned_weights/yolov5{}_finetuned.pt'.format(weight), source='local', force_reload=True, device=device)\n",
    "\n",
    "    for i, ratio in enumerate(pruning_ratios):\n",
    "        print('({}) pruning ratio: {}'.format(i+1, ratio))\n",
    "        pruned_model = apply_prune(model, pruning_ratio=ratio, real_pruning=real_pruning, mode=mode)\n",
    "        flops, nparams = flops_and_params(pruned_model)\n",
    "        print('FLOPs: %.3f GFLOPs, number of parameters: %d' % (flops / 1e9, nparams))\n",
    "        model_size = get_model_size(pruned_model)\n",
    "        metrics, times = evaluate(model=model, data_yaml='./data/glasses.yaml', device=device, half=half)\n",
    "        results.append([ratio, metrics['Precision'], metrics['Recall'], metrics['mAP@0.5'], metrics['mAP@0.5:0.95'], times])\n",
    "        inform.append([model_size, flops, nparams])\n",
    "        print('pruning ratio {} complete'.format(ratio))\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    return results, inform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#results, informs = evaluate_pruning(weight=weight, real_pruning=False, mode='Unstructured')\n",
    "\n",
    "def plot_results(weight, results, informs, real_pruning, mode):\n",
    "    folder = '../pruning_results_2'\n",
    "    # 데이터 추출\n",
    "    ratios = [r[0] for r in results]\n",
    "    precision_score = [r[1] for r in results]\n",
    "    recall_score = [r[2] for r in results]\n",
    "    map50_scores = [r[3] for r in results]\n",
    "    map50_95_scores = [r[4] for r in results]\n",
    "    inference_times = [r[5][1] for r in results]\n",
    "\n",
    "    model_sizes = [i[0] for i in informs]\n",
    "    flops = [i[1] for i in informs]\n",
    "    nparams = [i[2] for i in informs]\n",
    "\n",
    "    # 그래프 그리기\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(24,6))\n",
    "    ax[0].plot(ratios, map50_scores, marker='o', label='mAP@0.5')\n",
    "    ax[0].plot(ratios, map50_95_scores, marker='o', label='mAP@0.5:0.95')\n",
    "    ax[0].set_title(\"mAP\")\n",
    "    ax[0].set_xlabel(\"Pruning Ratio\")\n",
    "    ax[0].set_ylabel(\"Performance\")\n",
    "    ax[0].legend()\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ax[1].plot(ratios, precision_score, color='r', marker='o', label='precision')\n",
    "    ax[1].plot(ratios, recall_score, color='g', marker='o', label='recall')\n",
    "    ax[1].set_title(\"Precision and Recall\")\n",
    "    ax[1].set_xlabel(\"Pruning Ratio\")\n",
    "    ax[1].set_ylabel(\"Performance\")\n",
    "    ax[1].legend()\n",
    "    ax[1].grid(True)\n",
    "\n",
    "    ax[2].plot(ratios, inference_times, color='m', marker='o', label='precision')\n",
    "    ax[2].set_title(\"Inference Time\")\n",
    "    ax[2].set_xlabel(\"Pruning Ratio\")\n",
    "    ax[2].set_ylabel(\"time (ms)\")\n",
    "    ax[2].legend()\n",
    "    ax[2].grid(True)\n",
    "\n",
    "    fig.suptitle('YOLOv5{} {} {} Pruning Ratio vs Performance'.format(weight, 'Real' if real_pruning else 'Fake', mode))\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    plt.savefig(\"{}/YOLOv5{}_{}_{}_Performance.png\".format(folder, weight, 'Real' if real_pruning else 'Fake', mode))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(24,6))\n",
    "    ax[0].plot(ratios, model_sizes, 'bo-', label='model_size')\n",
    "    ax[0].set_title(\"Model Size (MB)\")\n",
    "    ax[0].set_xlabel(\"Pruning Ratio\")\n",
    "    ax[0].set_ylabel(\"MB\")\n",
    "    ax[0].set_ylim(0, 1.4 * model_sizes[0])\n",
    "    ax[0].legend()\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ax[1].plot(ratios, flops, 'ro-', label='GFLOPs')\n",
    "    ax[1].set_title(\"FLOPs (G)\")\n",
    "    ax[1].set_xlabel(\"Pruning Ratio\")\n",
    "    ax[1].set_ylabel(\"GFLOPs\")\n",
    "    ax[1].legend()\n",
    "    ax[1].grid(True)\n",
    "\n",
    "    ax[2].plot(ratios, nparams, 'go-', label='number of parameters(M)')\n",
    "    ax[2].set_title(\"Number of parameters (M)\")\n",
    "    ax[2].set_xlabel(\"Pruning Ratio\")\n",
    "    ax[2].set_ylabel(\"# parameters (M)\")\n",
    "    ax[2].legend()\n",
    "    ax[2].grid(True)\n",
    "\n",
    "    fig.suptitle('YOLOv5{} {} {} Pruning Ratio vs Information'.format(weight, 'Real' if real_pruning else 'Fake', mode))\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    plt.savefig(\"{}/YOLOv5{}_{}_{}_Information.png\".format(folder, weight, 'Real' if real_pruning else 'Fake', mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 'n'\n",
    "exp_list = [(False, 'Unstructured'), (False, 'Structured'), (True, 'Structured')]\n",
    "\n",
    "for real_pruning, mode in exp_list:\n",
    "    results, informs = evaluate_pruning(weight, real_pruning, mode)\n",
    "    plot_results(weight, results, informs, real_pruning, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
